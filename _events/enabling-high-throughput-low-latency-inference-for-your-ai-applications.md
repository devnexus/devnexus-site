---
questionAnswers: []
id: '1089159'
title: Enabling High-Throughput, Low-Latency Inference for Your AI Applications
description: "AI inference uses trained models—from small regressions to large language
  models—to make predictions on\r\nnew data. While modern models excel at reasoning
  and orchestration, calling them for every prediction can be\r\nslow and costly.
  In practice, many Spring applications use lightweight models for scoring, forecasting,
  or anomaly\r\ndetection—tasks where responses must be fast and local. In this session,
  you’ll learn how to combine Spring AI\r\nagents with local inference tools to get
  the best of both worlds. Using the ONNX (Open Neural Network\r\nExchange) standard,
  you can export models from Python and run them wherever your applications live—directly\r\nwithin
  your caching or data layer for immediate, in-context predictions. In this talk,
  we’ll look at an example using\r\nGemFire, showing how Spring developers can execute
  these models seamlessly in Java while keeping latency\r\nlow and throughput high.
  A Spring AI agent can then invoke these embedded models as tools, delegating simple\r\ndecisions
  locally and reserving specialized calls for complex reasoning. The result: intelligent,
  data-driven\r\napplications that deliver real-time predictions at the speed of cache."
startsAt: '2026-03-05T15:50:00'
endsAt: '2026-03-05T16:50:00'
isServiceSession: false
isPlenumSession: false
speakers:
- id: bd909527-212e-4863-b8f0-fd947aff7fa3
  name: Cora Iberkleid
categories:
- id: 107979
  name: Track
  categoryItems:
  - id: 425204
    name: Production Ready Spring
  sort: 0
roomId: 70250
room: Production Ready Spring
liveUrl:
recordingUrl:
status: Accepted
isInformed: true
isConfirmed: true
track: Production Ready Spring
slug: enabling-high-throughput-low-latency-inference-for-your-ai-applications

---
